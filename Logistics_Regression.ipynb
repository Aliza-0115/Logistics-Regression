{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Logistics Regression:"
      ],
      "metadata": {
        "id": "8g8XY9O-9KzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Logistics Regression:-"
      ],
      "metadata": {
        "id": "m3d1I2m59LWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "Ans- Simple Linear Regression (SLR) is a statistical method used to model the relationship between two continuous variables: a dependent variable (also known as the response or outcome variable) and an independent variable (also known as the predictor or explanatory variable).\n",
        "\n",
        "The purpose of SLR is to:\n",
        "\n",
        "1. 3Predict future values: Once the relationship is established, SLR can be used to predict the value of the dependent variable based on the value of the independent variable.\n",
        "\n",
        "2. Understand the relationship: It helps in understanding the strength and direction of the linear relationship between the two variables.\n",
        "\n",
        "3. Identify trends: It can reveal if there's a trend in the dependent variable as the independent variable changes."
      ],
      "metadata": {
        "id": "qptX8We39Mvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans- The key assumptions of Simple Linear Regression (SLR) are:\n",
        "\n",
        "1. Linearity: The relationship between the independent variable (X) and the dependent variable (Y) must be linear. This means that the change in Y for a one-unit change in X is constant.\n",
        "\n",
        "2. Independence of Errors: The errors (residuals) should be independent of each other. This assumption is often violated in time series data, where errors might be correlated over time.\n",
        "\n",
        "3. Homoscedasticity: The variance of the errors should be constant across all levels of the independent variable. In simpler terms, the spread of the residuals should be roughly the same along the regression line.\n",
        "\n",
        "4. Normality of Errors: The errors (residuals) should be normally distributed. While SLR doesn't strictly require the variables themselves to be normally distributed, the distribution of the errors around the regression line should be approximately normal. This assumption is particularly important for constructing confidence intervals and performing hypothesis tests.\n",
        "\n",
        "5. No Multicollinearity (for Multiple Linear Regression): Although not strictly an assumption for Simple Linear Regression (which only has one independent variable), it's a crucial assumption for Multiple Linear Regression. It states that the independent variables should not be highly correlated with each other.\n"
      ],
      "metadata": {
        "id": "mbEyUIwQ9jC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write the mathematical equation for a simple linear regression model and\n",
        "explain each term.\n",
        "\n",
        "Ans- The mathematical equation for a Simple Linear Regression (SLR) model is:\n",
        "\n",
        "[ Y = \\beta_0 + \\beta_1 X + \\epsilon ]\n",
        "\n",
        "Let's break down each term:\n",
        "\n",
        "( Y ): This is the dependent variable (also known as the response variable, outcome variable, or target variable). It's the variable we are trying to predict or explain.\n",
        "\n",
        "( X ): This is the independent variable (also known as the predictor variable, explanatory variable, or feature). It's the variable we use to predict ( Y ).\n",
        "\n",
        "( \\beta_0 ) (Beta-naught): This is the Y-intercept of the regression line. It represents the expected value of ( Y ) when ( X ) is 0.\n",
        "\n",
        "( \\beta_1 ) (Beta-one): This is the slope of the regression line. It represents the change in ( Y ) for every one-unit change in ( X ). It indicates the strength and direction of the linear relationship between ( X ) and ( Y ).\n",
        "\n",
        "( \\epsilon ) (Epsilon): This is the error term or residual. It represents the difference between the actual observed value of ( Y ) and the value predicted by the regression line. It accounts for all other factors influencing ( Y ) that are not included in ( X ), as well as random variation.\n",
        "\n"
      ],
      "metadata": {
        "id": "rIt5My5N98sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        "Ans- Example: Predicting House Prices based on Size\n",
        "\n",
        "Imagine you are a real estate agent, and you want to predict the selling price of a house based on its living area (square footage). You collect data from recent home sales in a particular neighborhood, noting the living area (in square feet) and the corresponding selling price (in dollars) for each house.\n",
        "\n",
        "Dependent Variable (Y): Selling Price of the house (the variable you want to predict).\n",
        "Independent Variable (X): Living Area of the house (the variable you use to predict the price).\n",
        "By applying Simple Linear Regression, you can:\n",
        "\n",
        "Model the relationship: You can establish a linear relationship between the living area and the selling price. For instance, the model might tell you that, on average, for every additional square foot, the house price increases by a certain amount.\n",
        "Predict prices: Once the model is built, if a new house comes on the market with a known living area, you can use your regression equation to estimate its likely selling price.\n",
        "Understand impact: You can quantify how much the selling price is influenced by the size of the house, holding other factors constant (though in a real scenario, you'd likely use multiple regression for more accuracy)."
      ],
      "metadata": {
        "id": "nMDWekTu-LN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the method of least squares in linear regression?\n",
        "\n",
        "Ans- The method of least squares is a fundamental technique used in linear regression to find the best-fitting line through a set of data points. The core idea is to minimize the sum of the squares of the differences between the observed (actual) values and the values predicted by the regression line.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "Objective: To find the line that best represents the linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "Residuals (Errors): For each data point, there's a vertical distance between the actual observed Y-value and the Y-value predicted by the regression line. This distance is called the residual or error (( \\epsilon ) in the equation).\n",
        "\n",
        "Squaring the Residuals: We square each residual for two main reasons:\n",
        "\n",
        "To prevent positive and negative errors from canceling each other out, which would lead to a misleading sum of zero even if there are large errors.\n",
        "To give more weight to larger errors, meaning the model will try harder to reduce substantial deviations.\n",
        "Sum of Squared Residuals (SSR): The method then sums all these squared residuals. The goal is to make this sum as small as possible.\n",
        "\n",
        "Minimization: Mathematically, the method of least squares involves taking the partial derivatives of the sum of squared residuals with respect to the regression coefficients (the intercept ( \\beta_0 ) and the slope ( \\beta_1 )) and setting them to zero. Solving these equations gives the values of ( \\beta_0 ) and ( \\beta_1 ) that define the line with the minimum sum of squared errors."
      ],
      "metadata": {
        "id": "1vM_BEn4-Yrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "Ans- Logistic Regression is a statistical model used for binary classification tasks, meaning it predicts the probability that an observation belongs to one of two classes (e.g., yes/no, true/false, success/failure). Unlike Linear Regression, which is used for continuous outcome variables, Logistic Regression uses a sigmoid (or logistic) function to map any real-valued input into a value between 0 and 1, which can then be interpreted as a probability.\n",
        "\n",
        "Differences:\n",
        "\n",
        "1. Type of Dependent Variable: Linear Regression predicts a continuous dependent variable (e.g., house price, temperature), while Logistic Regression predicts a categorical dependent variable, specifically a binary one (e.g., whether an email is spam or not).\n",
        "\n",
        "2. Output: Linear Regression outputs a continuous value. Logistic Regression outputs a probability score between 0 and 1.\n",
        "\n",
        "3. Equation/Function: Linear Regression uses a linear equation ($Y = \\beta_0 + \\beta_1 X + \\epsilon$$Y = \\beta_0 + \\beta_1 X + \\epsilon$). Logistic Regression uses the logistic function (sigmoid function) to transform the linear combination of predictors ($P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} $$P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} $).\n",
        "\n",
        "4. Assumptions: While both have assumptions, Logistic Regression does not assume linearity between the independent and dependent variables, nor does it assume homoscedasticity or normally distributed errors. It assumes a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
        "\n",
        "5. Cost Function: Linear Regression typically uses Mean Squared Error (MSE) as its cost function. Logistic Regression uses a log-likelihood or cross-entropy loss function."
      ],
      "metadata": {
        "id": "-JlIdTiO-rzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "\n",
        "Ans-Here are three common evaluation metrics for regression models:\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Description: MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average of the absolute differences between predicted and actual values.\n",
        "Why it's used: It's robust to outliers because it uses absolute values, and it gives an easily interpretable average error in the same units as the dependent variable.\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Description: MSE calculates the average of the squares of the errors. It's the sum of the squared differences between predicted and actual values, divided by the number of data points.\n",
        "Why it's used: MSE penalizes larger errors more heavily due to the squaring, making it sensitive to outliers. It is also often preferred in many statistical contexts because it corresponds to maximizing the likelihood of the data for normally distributed errors.\n",
        "\n",
        "R-squared (R²):\n",
        "\n",
        "Description: R-squared, also known as the coefficient of determination, represents the proportion of the variance in the dependent variable that can be predicted from the independent variables. It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "Why it's used: It provides a measure of how well the model explains the variability of the response data around its mean. A higher R² value means the model explains more of the variance."
      ],
      "metadata": {
        "id": "_EqhkSjRK84w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "Ans- The purpose of the R-squared (R²) metric in regression analysis is to represent the proportion of the variance in the dependent variable that can be predicted or explained by the independent variables in the model. In simpler terms, it tells you how well your regression model fits the observed data.\n",
        "\n",
        "Key aspects of its purpose include:\n",
        "\n",
        "Goodness of Fit: It provides a measure of how well the regression line approximates the real data points. A higher R² indicates a better fit.\n",
        "\n",
        "Explained Variance: It quantifies the percentage of the dependent variable's variability that is explained by the independent variable(s) in the model.\n",
        "\n",
        "Model Comparison (with caution): While a higher R² generally suggests a better model, it should be used with caution for model comparison, especially between models with different numbers of predictors. For such comparisons, adjusted R-squared is often preferred as it accounts for the number of predictors.\n",
        "\n",
        "An R² of 0.80, for example, means that 80% of the variation in the dependent variable can be explained by the independent variable(s) included in the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "vyPtMtmVLI3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write Python code to fit a simple linear regression model using scikit-learn\n",
        "and print the slope and intercept.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Ans- The Python code executed successfully! The output shows:\n",
        "\n",
        "Slope (Coefficient): 7.52 This means that for every one-unit increase in the independent variable (e.g., 'hours studied' in our example), the dependent variable (e.g., 'exam score') is predicted to increase by 7.52 units.\n",
        "Intercept: 25.67 This is the predicted value of the dependent variable when the independent variable is zero. In our example, it would mean that if someone studied 0 hours, their predicted exam score would be 25.67. This might not always be meaningful in real-world contexts, but it's a necessary component of the linear equation."
      ],
      "metadata": {
        "id": "x6a9CHNMLVxm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168d2059",
        "outputId": "e92d05f5-13b5-4289-d593-13225972cc2f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Generate some sample data\n",
        "# Let's say X is 'hours studied' and Y is 'exam score'\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1) # Independent variable\n",
        "Y = np.array([20, 40, 55, 60, 70, 75, 80, 85, 90, 95])    # Dependent variable\n",
        "\n",
        "# 2. Create a Linear Regression model object\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Fit the model to the data\n",
        "model.fit(X, Y)\n",
        "\n",
        "# 4. Print the slope (coefficient) and intercept\n",
        "print(f\"Slope (Coefficient): {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 7.52\n",
            "Intercept: 25.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "Ans- Interpreting the coefficients in a simple linear regression model is straightforward, as each coefficient has a specific meaning:\n",
        "\n",
        "The Slope ((\\beta_1)):\n",
        "\n",
        "Interpretation: The slope represents the change in the dependent variable (Y) for every one-unit increase in the independent variable (X). It quantifies the strength and direction of the linear relationship between X and Y.\n",
        "\n",
        "Example: If you're predicting house price (Y) based on square footage (X), and the slope is 100, it means that for every additional square foot, the house price is predicted to increase by $100.\n",
        "Sign: A positive slope indicates a positive relationship (as X increases, Y tends to increase), while a negative slope indicates a negative relationship (as X increases, Y tends to decrease).\n",
        "The Intercept ((\\beta_0)):\n",
        "\n",
        "Interpretation: The intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is zero.\n",
        "Example: In the house price example, if the intercept is $50,000, it would mean that a house with zero square footage is predicted to cost $50,000. While this might not be practically meaningful (a house can't have zero square footage), it's a necessary component of the regression line and helps to correctly position the line on the Y-axis.\n",
        "\n",
        "Practicality: It's important to consider if an X value of zero is meaningful in the context of your data. If it's outside the range of your observed data or conceptually impossible, the intercept might not have a practical interpretation but is still crucial for the model's accuracy within the observed range.\n"
      ],
      "metadata": {
        "id": "a8c66u4tLo0q"
      }
    }
  ]
}